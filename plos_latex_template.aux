\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plos2015}
\citation{pandora}
\citation{goodreads}
\citation{pythonmachinelearning}
\newlabel{sec:introduction}{{}{1}{Introduction}{section*.3}{}}
\citation{cam}
\citation{VanPoucke2016}
\citation{cam}
\citation{ISI:000337467400005}
\citation{ISI:000355882700012}
\citation{Gordon19851065}
\citation{Bou-Hamad201144}
\citation{Ishwaran20101056}
\citation{seerdoc}
\citation{bowles}
\citation{bowles}
\newlabel{sec:materialsandmethods}{{}{3}{Materials and Methods}{section*.4}{}}
\newlabel{subsec:dataprep}{{}{3}{Data preparation and preprocessing}{section*.5}{}}
\citation{census}
\citation{geocode}
\citation{elevation}
\citation{supp}
\citation{kuhn}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  {\bf  Example of the transformation of \color@box {}{white}{\texttt  {STATE-COUNTY RECODE}} to \color@box {}{white}{\texttt  {elevation}}, \color@box {}{white}{\texttt  {lat}}, and \color@box {}{white}{\texttt  {lng}}.}\relax }}{4}{table.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:nmhead}{{1}{4}{{\bf Example of the transformation of \codewhite {STATE-COUNTY RECODE} to \codewhite {elevation}, \codewhite {lat}, and \codewhite {lng}.}\relax }{table.caption.6}{}}
\newlabel{subsec:transformation}{{}{4}{Transformation of Censored Data for Machine Learning}{section*.7}{}}
\newlabel{eq:hhazard}{{1}{4}{Transformation of Censored Data for Machine Learning}{equation.0.1}{}}
\citation{kuhn}
\citation{downey}
\newlabel{eq:pmf}{{2}{5}{Transformation of Censored Data for Machine Learning}{equation.0.2}{}}
\newlabel{eq:cdf}{{3}{5}{Transformation of Censored Data for Machine Learning}{equation.0.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  {\bf  Example of four columns in an uncensored record in the untransformed dataset.}\relax }}{5}{table.caption.8}}
\newlabel{tab:originaldead}{{2}{5}{{\bf Example of four columns in an uncensored record in the untransformed dataset.}\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  {\bf  Example of four columns in a censored record in the untransformed dataset.}\relax }}{5}{table.caption.9}}
\newlabel{tab:originalalive}{{3}{5}{{\bf Example of four columns in a censored record in the untransformed dataset.}\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces  {\bf  Example of four columns in an uncensored record in the transformed dataset.}\relax }}{6}{table.caption.10}}
\newlabel{tab:transformeddead}{{4}{6}{{\bf Example of four columns in an uncensored record in the transformed dataset.}\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  {\bf  Example of four columns in a censored record in the transformed dataset.}\relax }}{6}{table.caption.11}}
\newlabel{tab:transformedalive}{{5}{6}{{\bf Example of four columns in a censored record in the transformed dataset.}\relax }{table.caption.11}{}}
\newlabel{subsec:traintest}{{}{6}{Training and Test Partitions}{section*.12}{}}
\citation{wisdom}
\citation{cassidy}
\citation{outliers}
\citation{supp}
\newlabel{sec:predmodels}{{}{7}{Prediction Models}{section*.13}{}}
\citation{rf}
\citation{kagglerf}
\citation{deeplearning}
\citation{toxicity}
\@writefile{toc}{\contentsline {paragraph}{Decision Trees and Random Forests}{8}{section*.14}}
\citation{keras}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Illustration of ensemble methods showing how a collection of base learners with poor accuracy can combine to produce an accurate ensemble learner.\relax }}{9}{figure.caption.15}}
\newlabel{fig:ensemble}{{1}{9}{Illustration of ensemble methods showing how a collection of base learners with poor accuracy can combine to produce an accurate ensemble learner.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Layer Perceptron Neural Networks}{9}{section*.16}}
\newlabel{sec:results}{{}{9}{Results}{section*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Example of the construction of the binary classifiers for 6, 12, and 60 months survival. A subject's hazard curve $\lambda (\mathbf  {X}, t)$ is predicted by the model for times out to 107 months. The survival curve is then readily computed as in Equation\nobreakspace  {}(\ref  {eq:cdf}). For this example, the 6-month and 12-month classifiers predict survival, while the 60-month classifier predicts expiry.\relax }}{10}{figure.caption.18}}
\newlabel{fig:survivalexample}{{2}{10}{Example of the construction of the binary classifiers for 6, 12, and 60 months survival. A subject's hazard curve $\lambda (\mathbf {X}, t)$ is predicted by the model for times out to 107 months. The survival curve is then readily computed as in Equation~(\ref {eq:cdf}). For this example, the 6-month and 12-month classifiers predict survival, while the 60-month classifier predicts expiry.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Survival Curve Error Estimates}{10}{section*.19}}
\citation{supp}
\newlabel{sec:performancemetrics}{{}{11}{Performance Metrics}{section*.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  {\bf  AUC values for the Random Forest and Neural Networks model binary classifiers derived from the full survival curve predictions; see text for details. The number of subjects that were used in the calculation of a given AUC score are given in parenthesis after the score. }\relax }}{11}{table.caption.21}}
\newlabel{tab:AUC}{{6}{11}{{\bf AUC values for the Random Forest and Neural Networks model binary classifiers derived from the full survival curve predictions; see text for details. The number of subjects that were used in the calculation of a given AUC score are given in parenthesis after the score. }\relax }{table.caption.21}{}}
\newlabel{subsec:agreement}{{}{11}{Model Agreement}{section*.22}{}}
\citation{supp}
\citation{flask}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  {\bf  Percentage agreement for the Random Forest and Neural Network classifiers for 6, 12, and 60 month survival predictions on the test data for each cancer type.}\relax }}{12}{table.caption.23}}
\newlabel{tab:agree}{{7}{12}{{\bf Percentage agreement for the Random Forest and Neural Network classifiers for 6, 12, and 60 month survival predictions on the test data for each cancer type.}\relax }{table.caption.23}{}}
\newlabel{sec:apps}{{}{12}{Survival Curve Prediction Apps}{section*.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for breast cancer. The plot shows the same quantity for the 12 and 60 months classifiers. It is apparent from the figures that the outliers are due to the neural network models predicting higher survival probablitlies than the random forest for some few cases. These differences were evaluated for the 3300 test patients in the breast cancer data.\relax }}{13}{figure.caption.24}}
\newlabel{fig:breastbox}{{3}{13}{Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for breast cancer. The plot shows the same quantity for the 12 and 60 months classifiers. It is apparent from the figures that the outliers are due to the neural network models predicting higher survival probablitlies than the random forest for some few cases. These differences were evaluated for the 3300 test patients in the breast cancer data.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for colon cancer. The plot shows the same quantity for the 12 and 60 months classifiers. It is apparent from the figures that the outliers are due to the neural network models predicting higher survival probablitlies than the random forest for some few cases. These differences were evaluated for the 5654 test patients in the colon cancer data.\relax }}{13}{figure.caption.25}}
\newlabel{fig:colonbox}{{4}{13}{Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for colon cancer. The plot shows the same quantity for the 12 and 60 months classifiers. It is apparent from the figures that the outliers are due to the neural network models predicting higher survival probablitlies than the random forest for some few cases. These differences were evaluated for the 5654 test patients in the colon cancer data.\relax }{figure.caption.25}{}}
\citation{kob4}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for lung cancer. The plot shows the same quantity for the 12 and 60 months classifiers. These differences were evaluated for the 5313 test patients in the lung cancer data. The Interquartile Ranges for lung cancer are visibly larger than those for breast cancer and colon cancer shown in fig\nobreakspace  {}\ref  {fig:breastbox} and fig\nobreakspace  {}\ref  {fig:colonbox}.\relax }}{14}{figure.caption.26}}
\newlabel{fig:lungbox}{{5}{14}{Box plots showing the distributions of the signed difference between the MLP model's prediction for the probability of surviving 6 months and the Random Forest model's prediction of the same quantity for lung cancer. The plot shows the same quantity for the 12 and 60 months classifiers. These differences were evaluated for the 5313 test patients in the lung cancer data. The Interquartile Ranges for lung cancer are visibly larger than those for breast cancer and colon cancer shown in fig~\ref {fig:breastbox} and fig~\ref {fig:colonbox}.\relax }{figure.caption.26}{}}
\citation{umass}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces \bf  {Example input data to the Colon Cancer neural network app \url  {https://github.com/doolingdavid/colon-cancer-nn-errors.git}.}\relax }}{15}{table.caption.28}}
\newlabel{tab:boston1940}{{8}{15}{\bf {Example input data to the Colon Cancer neural network app \url {https://github.com/doolingdavid/colon-cancer-nn-errors.git}.}\relax }{table.caption.28}{}}
\newlabel{sec:discussion}{{}{15}{Discussion}{section*.31}{}}
\citation{seermed}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Colon Cancer Survival Curve predicted from the data in Table\nobreakspace  {}(\ref  {tab:boston1940}) using the neural network web app \url  {https://github.com/doolingdavid/colon-cancer-nn-errors.git}.\relax }}{16}{figure.caption.29}}
\newlabel{fig:boston1940}{{6}{16}{Colon Cancer Survival Curve predicted from the data in Table~(\ref {tab:boston1940}) using the neural network web app \url {https://github.com/doolingdavid/colon-cancer-nn-errors.git}.\relax }{figure.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces {\bf  Example input data to the Lung Cancer random forest app \url  {https://github.com/doolingdavid/lung-cancer-rf-errors.git}.}\relax }}{17}{table.caption.30}}
\newlabel{tab:lungmaritalstatus}{{9}{17}{{\bf Example input data to the Lung Cancer random forest app \url {https://github.com/doolingdavid/lung-cancer-rf-errors.git}.}\relax }{table.caption.30}{}}
\newlabel{sec:supporting}{{}{17}{Supporting Information}{section*.32}{}}
\newlabel{sec:raw}{{}{17}{Raw SEER datafiles}{section*.33}{}}
\newlabel{sec:subsets}{{}{18}{Data Subsets}{section*.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces  {\bf  Filters applied to the Colon Cancer data.}\relax }}{18}{table.caption.35}}
\newlabel{tab:colonfilter}{{10}{18}{{\bf Filters applied to the Colon Cancer data.}\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces  {\bf  Filters applied to the Lung Cancer data.}\relax }}{18}{table.caption.36}}
\newlabel{tab:lungfilter}{{11}{18}{{\bf Filters applied to the Lung Cancer data.}\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces  {\bf  Filters applied to the Breast Cancer data.}\relax }}{19}{table.caption.37}}
\newlabel{tab:breastfilter}{{12}{19}{{\bf Filters applied to the Breast Cancer data.}\relax }{table.caption.37}{}}
\newlabel{Colon_Cancer_Feature_Selection}{{}{19}{Colon Cancer Feature Selection}{section*.38}{}}
\newlabel{Lung_Cancer_Feature_Selection}{{}{21}{Lung Cancer Feature Selection}{section*.39}{}}
\newlabel{Breast_Cancer_Feature_Selection}{{}{24}{Breast Cancer Feature Selection}{section*.40}{}}
\newlabel{subsec:pseudocode}{{}{25}{Pseudocode for the Data Transformation}{section*.41}{}}
\newlabel{subsec:breastrf}{{}{26}{Breast Random Forest Model Hyperparameters}{section*.42}{}}
\newlabel{subsec:colonrf}{{}{26}{Colon Random Forest Model Hyperparameters}{section*.43}{}}
\newlabel{subsec:lungrf}{{}{26}{Lung Random Forest Model Hyperparameters}{section*.44}{}}
\newlabel{subsec:breastnn}{{}{26}{Breast Neural Network Model Architecture}{section*.45}{}}
\newlabel{subsec:colonnn}{{}{27}{Colon Cancer Neural Network Model Architecture}{section*.46}{}}
\newlabel{subsec:lungnn}{{}{27}{Lung Cancer Neural Network Model Architecture}{section*.47}{}}
\bibdata{newbib}
\bibcite{pandora}{1}
\newlabel{S1_Video}{{}{28}{S1 Video}{section*.48}{}}
\newlabel{S1_Text}{{}{28}{S1 Text}{section*.49}{}}
\newlabel{S1_Fig}{{}{28}{S1 Fig}{section*.50}{}}
\newlabel{S2_Fig}{{}{28}{S2 Fig}{section*.51}{}}
\newlabel{S1_Table}{{}{28}{S1 Table}{section*.52}{}}
\bibcite{goodreads}{2}
\bibcite{pythonmachinelearning}{3}
\bibcite{cam}{4}
\bibcite{VanPoucke2016}{5}
\bibcite{ISI:000337467400005}{6}
\bibcite{ISI:000355882700012}{7}
\bibcite{Gordon19851065}{8}
\bibcite{Bou-Hamad201144}{9}
\bibcite{Ishwaran20101056}{10}
\bibcite{seerdoc}{11}
\bibcite{bowles}{12}
\bibcite{census}{13}
\bibcite{geocode}{14}
\bibcite{elevation}{15}
\bibcite{supp}{16}
\bibcite{kuhn}{17}
\bibcite{downey}{18}
\bibcite{wisdom}{19}
\bibcite{cassidy}{20}
\bibcite{outliers}{21}
\bibcite{rf}{22}
\bibcite{kagglerf}{23}
\bibcite{deeplearning}{24}
\bibcite{toxicity}{25}
\bibcite{keras}{26}
\bibcite{flask}{27}
\bibcite{kob4}{28}
\bibcite{umass}{29}
\bibcite{seermed}{30}
\newlabel{LastPage}{{}{30}{}{page.30}{}}
\xdef\lastpage@lastpage{30}
\xdef\lastpage@lastpageHy{30}
